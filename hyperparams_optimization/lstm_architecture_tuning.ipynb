{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from mylibs import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "xhU8Cskx6dTw"
   },
   "source": [
    "## Data\n",
    "Load, split and prepare the data. Documentation of the functions can be found in data/data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 32.00 GB\n",
      "maxCacheSize: 10.67 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from data.data import Data\n",
    "handler = Data()\n",
    "\n",
    "# load the data from pickle file one directory above the current directory\n",
    "data = handler.load('../artifacts_5s_128hz.pkl')\n",
    "\n",
    "# split dataset into train, val and test set\n",
    "train_split, val_split, test_split = handler.split(data) \n",
    "\n",
    "# prepare balanced train dataset for convolutional lstm as tensorflow dataset\n",
    "train = handler.prepare_data(train_split,\n",
    "                             balance=True, \n",
    "                             dataset=True, \n",
    "                             lstm=True, \n",
    "                             ccn=True)\n",
    "\n",
    "\n",
    "# prepare unbalanced validation dataset for convolutional lstm as tensorflow dataset\n",
    "val = handler.prepare_data(val_split, \n",
    "                           dataset=True, \n",
    "                           lstm=True, \n",
    "                           ccn=True)\n",
    "\n",
    "# prepare unbalanced test dataset for convolutional lstm as numpy arrays\n",
    "test_features, test_labels = handler.prepare_data(test_split, \n",
    "                                                  lstm=True, \n",
    "                                                  ccn=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all other metrics that we would like to track for each model architecture\n",
    "metrics = [\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'), \n",
    "      keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
    "      keras.metrics.CategoricalCrossentropy(name='loss'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "T5qFDNtvjZ9m"
   },
   "source": [
    "## Dynamic model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "oTmSYVtj2RHP"
   },
   "outputs": [],
   "source": [
    "from architectures.MultiLayerLSTM import MultiLayerLSTM\n",
    "from architectures.BidirectionalLSTM import BidirectionalLSTM\n",
    "\n",
    "def create_model(hparams):\n",
    "    \"\"\"\n",
    "    returns a model based on the selected hyperparameters\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        \"MultiLayerLSTM\": MultiLayerLSTM(num_hidden_units = hparams['HP_NUM_HIDDEN_UNITS'], \n",
    "                                        num_lstm_layers=hparams['HP_NUM_LSTM_LAYERS'],\n",
    "                                        num_dense_units = hparams['HP_NUM_DENSE_LAYERS'],\n",
    "                                        num_dense_layers = hparams['HP_NUM_DENSE_UNITS'],\n",
    "                                        num_conv_layers=hparams['HP_NUM_CONV_LAYERS'],\n",
    "                                        increase=hparams['HP_INCREASE_UNITS_PER_LSTM_LAYER']),\n",
    "        \n",
    "        \"BidirectionalLSTM\": BidirectionalLSTM(num_hidden_units=hparams['HP_NUM_HIDDEN_UNITS'],\n",
    "                                             num_bidirectional_layers=hparams['HP_NUM_LSTM_LAYERS'],\n",
    "                                             num_dense_units = hparams['HP_NUM_DENSE_LAYERS'],\n",
    "                                             num_dense_layers = hparams['HP_NUM_DENSE_UNITS'],\n",
    "                                             num_conv_layers=hparams['HP_NUM_CONV_LAYERS'],\n",
    "                                             increase=hparams['HP_INCREASE_UNITS_PER_LSTM_LAYER'])\n",
    "    }\n",
    "\n",
    "    return models.get(hparams['HP_MODEL_ARCHITECTURE'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "vDinxqmPjcRT"
   },
   "outputs": [],
   "source": [
    "def run_experiment(hparams, logdir,savedir,checkpointdir,metrics):\n",
    "    \"\"\"\n",
    "    builds, compiles, trains and evaluates a model with certain architectual hyperparameters\n",
    "\n",
    "    Args:\n",
    "        hparams: selected hyperparameters\n",
    "        logdir: directory for logs\n",
    "        savedir: directors for saving the model\n",
    "        checkpointdir: direcotry for model checkpoints\n",
    "        metrics: a list of metrics we want to track\n",
    "\n",
    "    Returns:\n",
    "        accuracy of trained model evaluated on test data set\n",
    "    \"\"\"\n",
    "    model = create_model(hparams)\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        metrics=metrics)\n",
    "\n",
    "\n",
    "    model.fit(train,\n",
    "              batch_size=64,\n",
    "              epochs=1,\n",
    "              validation_data=val,\n",
    "              verbose=0, # no output during training\n",
    "              callbacks=[tf.keras.callbacks.TensorBoard(logdir),  # log metrics\n",
    "                        hp.KerasCallback(logdir, hparams),  # log hparams\n",
    "                        tf.keras.callbacks.ModelCheckpoint(filepath= os.path.join(checkpointdir, \"ckpt_{epoch}\") ,monitor='val_loss',save_weights_only=True), # save checkpoints when val loss goes down\n",
    "                        keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)], # early stopping in the case that loss doesnt go down for 3 epochs\n",
    "              ) \n",
    "    results = model.evaluate(test_features, test_labels)\n",
    "    model.save(savedir)\n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(hparams_dict, logdir):\n",
    "    \"\"\"\n",
    "    train models on the selected hyperparamers and log the metrics, checkpoints and trained model in logidr\n",
    "    \"\"\"\n",
    "    session_num = 0\n",
    "    for num_lstm_layers in hparams_dict['HP_NUM_LSTM_LAYERS'].domain.values:\n",
    "        for num_hidden_units in hparams_dict['HP_NUM_HIDDEN_UNITS'].domain.values:\n",
    "            for num_conv_layers in hparams_dict['HP_NUM_CONV_LAYERS'].domain.values:\n",
    "                for num_dense_layers in hparams_dict['HP_NUM_DENSE_LAYERS'].domain.values:\n",
    "                    for num_dense_units in hparams_dict['HP_NUM_DENSE_UNITS'].domain.values:\n",
    "                        for increase in hparams_dict['HP_INCREASE_UNITS_PER_LSTM_LAYER'].domain.values:\n",
    "                            for model_architecture in hparams_dict['HP_MODEL_ARCHITECTURE'].domain.values:\n",
    "                                hparams = {\n",
    "                                    'HP_MODEL_ARCHITECTURE': model_architecture,\n",
    "                                    'HP_NUM_LSTM_LAYERS': num_lstm_layers,\n",
    "                                    'HP_NUM_HIDDEN_UNITS': num_hidden_units,\n",
    "                                    'HP_NUM_CONV_LAYERS': num_conv_layers,\n",
    "                                    'HP_NUM_DENSE_LAYERS': num_dense_layers,\n",
    "                                    'HP_NUM_DENSE_UNITS': num_dense_units,\n",
    "                                    'HP_INCREASE_UNITS_PER_LSTM_LAYER': increase\n",
    "                                }\n",
    "                                run_name = \"run-%d\" % session_num\n",
    "                                print('--- Starting trial: %s' % run_name)\n",
    "                                print({h: hparams[h] for h in hparams})\n",
    "                                results = run_experiment(logdir=logdir+'hparam_tuning/' + run_name, \n",
    "                                              hparams=hparams, \n",
    "                                              savedir=logdir+'models/'+run_name, \n",
    "                                              checkpointdir=logdir+'checkpoints'+run_name,\n",
    "                                              metrics=metrics)\n",
    "                                session_num += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search 1 \n",
    "Plain MultiLayerLSTM and BidirectionalLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HParam(name='model_architecture', domain=Discrete(['BidirectionalLSTM', 'MultiLayerLSTM']), display_name=None, description=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HP_MODEL_ARCHITECTURE = hp.HParam('model_architecture', hp.Discrete(['MultiLayerLSTM', 'BidirectionalLSTM'])) # set model architecture as hyperparameter\n",
    "HP_NUM_LSTM_LAYERS = hp.HParam('num_lstm_layers', hp.Discrete([1, 2, 4])) # try out 1, 2 or 4 layers for architecture\n",
    "HP_NUM_HIDDEN_UNITS = hp.HParam('num_hidden_units', hp.Discrete([32, 64, 128, 256])) # try out different hidden_units\n",
    "HP_NUM_DENSE_UNITS = hp.HParam('num_dense_units', hp.Discrete([0])) # try out different number of units for dense layer\n",
    "HP_NUM_DENSE_LAYERS = hp.HParam('num_dense_layers', hp.Discrete([0])) # try out different number of dense layers\n",
    "HP_INCREASE_UNITS_PER_LSTM_LAYER = hp.HParam('increase_units_per_lstm_layer', hp.Discrete([0])) # increase number of units per lstm layer\n",
    "\n",
    "hparams_dict_1 = {\n",
    "    'HP_MODEL_ARCHITECTURE': HP_MODEL_ARCHITECTURE,\n",
    "    'HP_NUM_LSTM_LAYERS': HP_NUM_LSTM_LAYERS,\n",
    "    'HP_NUM_HIDDEN_UNITS': HP_NUM_HIDDEN_UNITS,\n",
    "    'HP_NUM_DENSE_UNITS': HP_NUM_DENSE_UNITS,\n",
    "    'HP_NUM_DENSE_LAYERS': HP_NUM_DENSE_LAYERS,\n",
    "    'HP_INCREASE_UNITS_PER_LSTM_LAYER': HP_INCREASE_UNITS_PER_LSTM_LAYER\n",
    "}\n",
    "\n",
    "search(logdir='logs1-1/', hparams_dict=hparams_dict_1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search 2\n",
    "plain MultiLayerLSTM and BidirectionalLSTM but with increasing hidden units per lstm layer, every lstm layer get twice more hidden units than layer before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_MODEL_ARCHITECTURE = hp.HParam('model_architecture', hp.Discrete(['MultiLayerLSTM', 'BidirectionalLSTM'])) # set model architecture as hyperparameter\n",
    "HP_NUM_LSTM_LAYERS = hp.HParam('num_lstm_layers', hp.Discrete([1, 2, 4])) # try out 1, 2 or 4 layers for architecture\n",
    "HP_NUM_HIDDEN_UNITS = hp.HParam('num_hidden_units', hp.Discrete([32, 64, 128, 256])) # try out different hidden_units\n",
    "HP_NUM_DENSE_UNITS = hp.HParam('num_dense_units', hp.Discrete([0])) # try out different number of units for dense layer\n",
    "HP_NUM_DENSE_LAYERS = hp.HParam('num_dense_layers', hp.Discrete([0])) # try out different number of dense layers\n",
    "HP_INCREASE_UNITS_PER_LSTM_LAYER = hp.HParam('increase_units_per_lstm_layer', hp.Discrete([1])) # increase number of units per lstm layer\n",
    "\n",
    "hparams_dict_2 = {\n",
    "    'HP_MODEL_ARCHITECTURE': HP_MODEL_ARCHITECTURE,\n",
    "    'HP_NUM_LSTM_LAYERS': HP_NUM_LSTM_LAYERS,\n",
    "    'HP_NUM_HIDDEN_UNITS': HP_NUM_HIDDEN_UNITS,\n",
    "    'HP_NUM_DENSE_UNITS': HP_NUM_DENSE_UNITS,\n",
    "    'HP_NUM_DENSE_LAYERS': HP_NUM_DENSE_LAYERS,\n",
    "    'HP_INCREASE_UNITS_PER_LSTM_LAYER': HP_INCREASE_UNITS_PER_LSTM_LAYER\n",
    "}\n",
    "\n",
    "search(logdir='logs1-2/', hparams_dict=hparams_dict_2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_MODEL_ARCHITECTURE = hp.HParam('model_architecture', hp.Discrete(['MultiLayerLSTM', 'BidirectionalLSTM'])) # set model architecture as hyperparameter\n",
    "HP_NUM_LSTM_LAYERS = hp.HParam('num_lstm_layers', hp.Discrete([1, 2, 4])) # try out 1, 2 or 4 layers for architecture\n",
    "HP_NUM_HIDDEN_UNITS = hp.HParam('num_hidden_units', hp.Discrete([32, 64, 128, 256])) # try out different hidden_units\n",
    "HP_NUM_DENSE_UNITS = hp.HParam('num_dense_units', hp.Discrete([128, 256])) # try out different number of units for dense layer\n",
    "HP_NUM_DENSE_LAYERS = hp.HParam('num_dense_layers', hp.Discrete([1,2,4])) # try out different number of dense layers\n",
    "HP_INCREASE_UNITS_PER_LSTM_LAYER = hp.HParam('increase_units_per_lstm_layer', hp.Discrete([0])) # increase number of units per lstm layer\n",
    "\n",
    "hparams_dict_3 = {\n",
    "    'HP_MODEL_ARCHITECTURE': HP_MODEL_ARCHITECTURE,\n",
    "    'HP_NUM_LSTM_LAYERS': HP_NUM_LSTM_LAYERS,\n",
    "    'HP_NUM_HIDDEN_UNITS': HP_NUM_HIDDEN_UNITS,\n",
    "    'HP_NUM_DENSE_UNITS': HP_NUM_DENSE_UNITS,\n",
    "    'HP_NUM_DENSE_LAYERS': HP_NUM_DENSE_LAYERS,\n",
    "    'HP_INCREASE_UNITS_PER_LSTM_LAYER': HP_INCREASE_UNITS_PER_LSTM_LAYER\n",
    "}\n",
    "\n",
    "search(logdir='logs1-2/', hparams_dict=hparams_dict_3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_MODEL_ARCHITECTURE = hp.HParam('model_architecture', hp.Discrete(['MultiLayerLSTM', 'BidirectionalLSTM'])) # set model architecture as hyperparameter\n",
    "HP_NUM_LSTM_LAYERS = hp.HParam('num_lstm_layers', hp.Discrete([1, 2, 4])) # try out 1, 2 or 4 layers for architecture\n",
    "HP_NUM_HIDDEN_UNITS = hp.HParam('num_hidden_units', hp.Discrete([32, 64, 128, 256])) # try out different hidden_units\n",
    "HP_NUM_CONV_LAYERS = hp.HParam('num_conv_layers', hp.Discrete([1,2,3,4,5])) # try out different amounts of conv layers --> connected to filter size\n",
    "HP_NUM_DENSE_UNITS = hp.HParam('num_dense_units', hp.Discrete([0])) # try out different number of units for dense layer\n",
    "HP_NUM_DENSE_LAYERS = hp.HParam('num_dense_layers', hp.Discrete([0])) # try out different number of dense layers\n",
    "HP_INCREASE_UNITS_PER_LSTM_LAYER = hp.HParam('increase_units_per_lstm_layer', hp.Discrete([0])) # increase number of units per lstm layer\n",
    "\n",
    "hparams_dict_4 = {\n",
    "    'HP_MODEL_ARCHITECTURE': HP_MODEL_ARCHITECTURE,\n",
    "    'HP_NUM_LSTM_LAYERS': HP_NUM_LSTM_LAYERS,\n",
    "    'HP_NUM_HIDDEN_UNITS': HP_NUM_HIDDEN_UNITS,\n",
    "    'HP_NUM_CONV_LAYERS': HP_NUM_CONV_LAYERS,\n",
    "    'HP_NUM_DENSE_UNITS': HP_NUM_DENSE_UNITS,\n",
    "    'HP_NUM_DENSE_LAYERS': HP_NUM_DENSE_LAYERS,\n",
    "    'HP_INCREASE_UNITS_PER_LSTM_LAYER': HP_INCREASE_UNITS_PER_LSTM_LAYER\n",
    "}\n",
    "\n",
    "search(logdir='logs1-4/', hparams_dict=hparams_dict_4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial: run-0\n",
      "{'HP_MODEL_ARCHITECTURE': 'BidirectionalLSTM', 'HP_NUM_LSTM_LAYERS': 1, 'HP_NUM_HIDDEN_UNITS': 8, 'HP_NUM_CONV_LAYERS': 1, 'HP_NUM_DENSE_LAYERS': 0, 'HP_NUM_DENSE_UNITS': 640, 'HP_INCREASE_UNITS_PER_LSTM_LAYER': 0}\n",
      "Tensor(\"bidirectional_lstm/Cast:0\", shape=(None, 640, 19, 1), dtype=float32)\n",
      "Tensor(\"bidirectional_lstm/time_distributed_3/Reshape_1:0\", shape=(None, 640, 17, 8), dtype=float32)\n",
      "Tensor(\"bidirectional_lstm/time_distributed_4/Reshape_1:0\", shape=(None, 640, 8, 8), dtype=float32)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/leonackermann/Desktop/EEG-Artifact-Detection/hyperparams_optimization/lstm_architecture_tuning.ipynb Cell 22\u001b[0m in \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/leonackermann/Desktop/EEG-Artifact-Detection/hyperparams_optimization/lstm_architecture_tuning.ipynb#X30sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m HP_INCREASE_UNITS_PER_LSTM_LAYER \u001b[39m=\u001b[39m hp\u001b[39m.\u001b[39mHParam(\u001b[39m'\u001b[39m\u001b[39mincrease_units_per_lstm_layer\u001b[39m\u001b[39m'\u001b[39m, hp\u001b[39m.\u001b[39mDiscrete([\u001b[39m0\u001b[39m])) \u001b[39m# increase number of units per lstm layer\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/leonackermann/Desktop/EEG-Artifact-Detection/hyperparams_optimization/lstm_architecture_tuning.ipynb#X30sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m hparams_dict_5 \u001b[39m=\u001b[39m {\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonackermann/Desktop/EEG-Artifact-Detection/hyperparams_optimization/lstm_architecture_tuning.ipynb#X30sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mHP_MODEL_ARCHITECTURE\u001b[39m\u001b[39m'\u001b[39m: HP_MODEL_ARCHITECTURE,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonackermann/Desktop/EEG-Artifact-Detection/hyperparams_optimization/lstm_architecture_tuning.ipynb#X30sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mHP_NUM_LSTM_LAYERS\u001b[39m\u001b[39m'\u001b[39m: HP_NUM_LSTM_LAYERS,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonackermann/Desktop/EEG-Artifact-Detection/hyperparams_optimization/lstm_architecture_tuning.ipynb#X30sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mHP_INCREASE_UNITS_PER_LSTM_LAYER\u001b[39m\u001b[39m'\u001b[39m: HP_INCREASE_UNITS_PER_LSTM_LAYER\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonackermann/Desktop/EEG-Artifact-Detection/hyperparams_optimization/lstm_architecture_tuning.ipynb#X30sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m }\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/leonackermann/Desktop/EEG-Artifact-Detection/hyperparams_optimization/lstm_architecture_tuning.ipynb#X30sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m search(logdir\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mlogs1-5/\u001b[39;49m\u001b[39m'\u001b[39;49m, hparams_dict\u001b[39m=\u001b[39;49mhparams_dict_5)\n",
      "\u001b[1;32m/Users/leonackermann/Desktop/EEG-Artifact-Detection/hyperparams_optimization/lstm_architecture_tuning.ipynb Cell 22\u001b[0m in \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonackermann/Desktop/EEG-Artifact-Detection/hyperparams_optimization/lstm_architecture_tuning.ipynb#X30sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m--- Starting trial: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m run_name)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonackermann/Desktop/EEG-Artifact-Detection/hyperparams_optimization/lstm_architecture_tuning.ipynb#X30sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mprint\u001b[39m({h: hparams[h] \u001b[39mfor\u001b[39;00m h \u001b[39min\u001b[39;00m hparams})\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/leonackermann/Desktop/EEG-Artifact-Detection/hyperparams_optimization/lstm_architecture_tuning.ipynb#X30sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m results \u001b[39m=\u001b[39m run(logdir\u001b[39m=\u001b[39;49mlogdir\u001b[39m+\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mhparam_tuning/\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m+\u001b[39;49m run_name, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonackermann/Desktop/EEG-Artifact-Detection/hyperparams_optimization/lstm_architecture_tuning.ipynb#X30sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m               hparams\u001b[39m=\u001b[39;49mhparams, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonackermann/Desktop/EEG-Artifact-Detection/hyperparams_optimization/lstm_architecture_tuning.ipynb#X30sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m               savedir\u001b[39m=\u001b[39;49mlogdir\u001b[39m+\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmodels/\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m+\u001b[39;49mrun_name, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonackermann/Desktop/EEG-Artifact-Detection/hyperparams_optimization/lstm_architecture_tuning.ipynb#X30sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m               checkpointdir\u001b[39m=\u001b[39;49mlogdir\u001b[39m+\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcheckpoints\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m+\u001b[39;49mrun_name,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonackermann/Desktop/EEG-Artifact-Detection/hyperparams_optimization/lstm_architecture_tuning.ipynb#X30sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m               metrics\u001b[39m=\u001b[39;49mmetrics)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonackermann/Desktop/EEG-Artifact-Detection/hyperparams_optimization/lstm_architecture_tuning.ipynb#X30sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m session_num \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[1;32m/Users/leonackermann/Desktop/EEG-Artifact-Detection/hyperparams_optimization/lstm_architecture_tuning.ipynb Cell 22\u001b[0m in \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonackermann/Desktop/EEG-Artifact-Detection/hyperparams_optimization/lstm_architecture_tuning.ipynb#X30sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m model \u001b[39m=\u001b[39m build_model(hparams)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonackermann/Desktop/EEG-Artifact-Detection/hyperparams_optimization/lstm_architecture_tuning.ipynb#X30sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonackermann/Desktop/EEG-Artifact-Detection/hyperparams_optimization/lstm_architecture_tuning.ipynb#X30sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonackermann/Desktop/EEG-Artifact-Detection/hyperparams_optimization/lstm_architecture_tuning.ipynb#X30sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     loss\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlosses\u001b[39m.\u001b[39mBinaryCrossentropy(),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonackermann/Desktop/EEG-Artifact-Detection/hyperparams_optimization/lstm_architecture_tuning.ipynb#X30sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     metrics\u001b[39m=\u001b[39mmetrics)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/leonackermann/Desktop/EEG-Artifact-Detection/hyperparams_optimization/lstm_architecture_tuning.ipynb#X30sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(train,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonackermann/Desktop/EEG-Artifact-Detection/hyperparams_optimization/lstm_architecture_tuning.ipynb#X30sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m           batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonackermann/Desktop/EEG-Artifact-Detection/hyperparams_optimization/lstm_architecture_tuning.ipynb#X30sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m           epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonackermann/Desktop/EEG-Artifact-Detection/hyperparams_optimization/lstm_architecture_tuning.ipynb#X30sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m           validation_data\u001b[39m=\u001b[39;49mval,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonackermann/Desktop/EEG-Artifact-Detection/hyperparams_optimization/lstm_architecture_tuning.ipynb#X30sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m           verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, \u001b[39m# no output during training\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonackermann/Desktop/EEG-Artifact-Detection/hyperparams_optimization/lstm_architecture_tuning.ipynb#X30sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m           callbacks\u001b[39m=\u001b[39;49m[tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mcallbacks\u001b[39m.\u001b[39;49mTensorBoard(logdir),  \u001b[39m# log metrics\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonackermann/Desktop/EEG-Artifact-Detection/hyperparams_optimization/lstm_architecture_tuning.ipynb#X30sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m                     hp\u001b[39m.\u001b[39;49mKerasCallback(logdir, hparams),  \u001b[39m# log hparams\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonackermann/Desktop/EEG-Artifact-Detection/hyperparams_optimization/lstm_architecture_tuning.ipynb#X30sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m                     tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mcallbacks\u001b[39m.\u001b[39;49mModelCheckpoint(filepath\u001b[39m=\u001b[39;49m os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(checkpointdir, \u001b[39m\"\u001b[39;49m\u001b[39mckpt_\u001b[39;49m\u001b[39m{epoch}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m) ,monitor\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mval_loss\u001b[39;49m\u001b[39m'\u001b[39;49m,save_weights_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m), \u001b[39m# save checkpoints when val loss goes down\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonackermann/Desktop/EEG-Artifact-Detection/hyperparams_optimization/lstm_architecture_tuning.ipynb#X30sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m                     keras\u001b[39m.\u001b[39;49mcallbacks\u001b[39m.\u001b[39;49mEarlyStopping(monitor\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mval_loss\u001b[39;49m\u001b[39m'\u001b[39;49m, patience\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m, restore_best_weights\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)], \u001b[39m# early stopping in the case that loss doesnt go down for 3 epochs\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonackermann/Desktop/EEG-Artifact-Detection/hyperparams_optimization/lstm_architecture_tuning.ipynb#X30sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m           ) \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonackermann/Desktop/EEG-Artifact-Detection/hyperparams_optimization/lstm_architecture_tuning.ipynb#X30sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m results \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(test_features, test_labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonackermann/Desktop/EEG-Artifact-Detection/hyperparams_optimization/lstm_architecture_tuning.ipynb#X30sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m model\u001b[39m.\u001b[39msave(savedir)\n",
      "File \u001b[0;32m~/miniforge3/envs/iannwtf/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniforge3/envs/iannwtf/lib/python3.10/site-packages/keras/engine/training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1557\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1558\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1561\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1562\u001b[0m ):\n\u001b[1;32m   1563\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1564\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1565\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1566\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniforge3/envs/iannwtf/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniforge3/envs/iannwtf/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniforge3/envs/iannwtf/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:963\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    960\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    961\u001b[0m   \u001b[39m# This is the first call of __call__, so we have to initialize.\u001b[39;00m\n\u001b[1;32m    962\u001b[0m   initializers \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 963\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initialize(args, kwds, add_initializers_to\u001b[39m=\u001b[39;49minitializers)\n\u001b[1;32m    964\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    965\u001b[0m   \u001b[39m# At this point we know that the initialization is complete (or less\u001b[39;00m\n\u001b[1;32m    966\u001b[0m   \u001b[39m# interestingly an exception was raised) so we no longer need a lock.\u001b[39;00m\n\u001b[1;32m    967\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniforge3/envs/iannwtf/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:785\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lifted_initializer_graph \u001b[39m=\u001b[39m lifted_initializer_graph\n\u001b[1;32m    783\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph_deleter \u001b[39m=\u001b[39m FunctionDeleter(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lifted_initializer_graph)\n\u001b[1;32m    784\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_concrete_stateful_fn \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 785\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateful_fn\u001b[39m.\u001b[39;49m_get_concrete_function_internal_garbage_collected(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    786\u001b[0m         \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds))\n\u001b[1;32m    788\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvalid_creator_scope\u001b[39m(\u001b[39m*\u001b[39munused_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39munused_kwds):\n\u001b[1;32m    789\u001b[0m   \u001b[39m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/iannwtf/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2523\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2521\u001b[0m   args, kwargs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   2522\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m-> 2523\u001b[0m   graph_function, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_function(args, kwargs)\n\u001b[1;32m   2524\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\n",
      "File \u001b[0;32m~/miniforge3/envs/iannwtf/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2760\u001b[0m, in \u001b[0;36mFunction._maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2758\u001b[0m   \u001b[39m# Only get placeholders for arguments, not captures\u001b[39;00m\n\u001b[1;32m   2759\u001b[0m   args, kwargs \u001b[39m=\u001b[39m placeholder_dict[\u001b[39m\"\u001b[39m\u001b[39margs\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m-> 2760\u001b[0m graph_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_graph_function(args, kwargs)\n\u001b[1;32m   2762\u001b[0m graph_capture_container \u001b[39m=\u001b[39m graph_function\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39m_capture_func_lib  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   2763\u001b[0m \u001b[39m# Maintain the list of all captures\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/iannwtf/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2670\u001b[0m, in \u001b[0;36mFunction._create_graph_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2665\u001b[0m missing_arg_names \u001b[39m=\u001b[39m [\n\u001b[1;32m   2666\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (arg, i) \u001b[39mfor\u001b[39;00m i, arg \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(missing_arg_names)\n\u001b[1;32m   2667\u001b[0m ]\n\u001b[1;32m   2668\u001b[0m arg_names \u001b[39m=\u001b[39m base_arg_names \u001b[39m+\u001b[39m missing_arg_names\n\u001b[1;32m   2669\u001b[0m graph_function \u001b[39m=\u001b[39m ConcreteFunction(\n\u001b[0;32m-> 2670\u001b[0m     func_graph_module\u001b[39m.\u001b[39;49mfunc_graph_from_py_func(\n\u001b[1;32m   2671\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name,\n\u001b[1;32m   2672\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_python_function,\n\u001b[1;32m   2673\u001b[0m         args,\n\u001b[1;32m   2674\u001b[0m         kwargs,\n\u001b[1;32m   2675\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_signature,\n\u001b[1;32m   2676\u001b[0m         autograph\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph,\n\u001b[1;32m   2677\u001b[0m         autograph_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph_options,\n\u001b[1;32m   2678\u001b[0m         arg_names\u001b[39m=\u001b[39;49marg_names,\n\u001b[1;32m   2679\u001b[0m         capture_by_value\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_capture_by_value),\n\u001b[1;32m   2680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_attributes,\n\u001b[1;32m   2681\u001b[0m     spec\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_spec,\n\u001b[1;32m   2682\u001b[0m     \u001b[39m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[1;32m   2683\u001b[0m     \u001b[39m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[1;32m   2684\u001b[0m     \u001b[39m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[1;32m   2685\u001b[0m     \u001b[39m# ConcreteFunction.\u001b[39;00m\n\u001b[1;32m   2686\u001b[0m     shared_func_graph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m   2687\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\n",
      "File \u001b[0;32m~/miniforge3/envs/iannwtf/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:1247\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1244\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1245\u001b[0m   _, original_func \u001b[39m=\u001b[39m tf_decorator\u001b[39m.\u001b[39munwrap(python_func)\n\u001b[0;32m-> 1247\u001b[0m func_outputs \u001b[39m=\u001b[39m python_func(\u001b[39m*\u001b[39;49mfunc_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfunc_kwargs)\n\u001b[1;32m   1249\u001b[0m \u001b[39m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[1;32m   1250\u001b[0m \u001b[39m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[1;32m   1251\u001b[0m func_outputs \u001b[39m=\u001b[39m nest\u001b[39m.\u001b[39mmap_structure(\n\u001b[1;32m   1252\u001b[0m     convert, func_outputs, expand_composites\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniforge3/envs/iannwtf/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:677\u001b[0m, in \u001b[0;36mFunction._defun_with_scope.<locals>.wrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[39mwith\u001b[39;00m default_graph\u001b[39m.\u001b[39m_variable_creator_scope(scope, priority\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m):  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    674\u001b[0m   \u001b[39m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[1;32m    675\u001b[0m   \u001b[39m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[1;32m    676\u001b[0m   \u001b[39mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[0;32m--> 677\u001b[0m     out \u001b[39m=\u001b[39m weak_wrapped_fn()\u001b[39m.\u001b[39;49m__wrapped__(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    678\u001b[0m   \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniforge3/envs/iannwtf/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:1222\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1220\u001b[0m \u001b[39m# TODO(mdan): Push this block higher in tf.function's call stack.\u001b[39;00m\n\u001b[1;32m   1221\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1222\u001b[0m   \u001b[39mreturn\u001b[39;00m autograph\u001b[39m.\u001b[39;49mconverted_call(\n\u001b[1;32m   1223\u001b[0m       original_func,\n\u001b[1;32m   1224\u001b[0m       args,\n\u001b[1;32m   1225\u001b[0m       kwargs,\n\u001b[1;32m   1226\u001b[0m       options\u001b[39m=\u001b[39;49mautograph\u001b[39m.\u001b[39;49mConversionOptions(\n\u001b[1;32m   1227\u001b[0m           recursive\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1228\u001b[0m           optional_features\u001b[39m=\u001b[39;49mautograph_options,\n\u001b[1;32m   1229\u001b[0m           user_requested\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1230\u001b[0m       ))\n\u001b[1;32m   1231\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m   1232\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniforge3/envs/iannwtf/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:439\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    438\u001b[0m   \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39;49meffective_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    440\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39meffective_args)\n",
      "File \u001b[0;32m/var/folders/2q/hwnn9141093b7bkbnd4bm7tc0000gn/T/__autograph_generated_filemaor481c.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(step_function), (ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m), ag__\u001b[39m.\u001b[39;49mld(iterator)), \u001b[39mNone\u001b[39;49;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/iannwtf/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:377\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    374\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[1;32m    376\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m options\u001b[39m.\u001b[39muser_requested \u001b[39mand\u001b[39;00m conversion\u001b[39m.\u001b[39mis_allowlisted(f):\n\u001b[0;32m--> 377\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[1;32m    379\u001b[0m \u001b[39m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[39m# call conversion from generated code while in nonrecursive mode. In that\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[39m# case we evidently don't want to recurse, but we still have to convert\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[39m# things like builtins.\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m options\u001b[39m.\u001b[39minternal_convert_user_code:\n",
      "File \u001b[0;32m~/miniforge3/envs/iannwtf/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:459\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    458\u001b[0m   \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m~/miniforge3/envs/iannwtf/lib/python3.10/site-packages/keras/engine/training.py:1146\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function\u001b[0;34m(model, iterator)\u001b[0m\n\u001b[1;32m   1142\u001b[0m     run_step \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mfunction(\n\u001b[1;32m   1143\u001b[0m         run_step, jit_compile\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, reduce_retracing\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1144\u001b[0m     )\n\u001b[1;32m   1145\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(iterator)\n\u001b[0;32m-> 1146\u001b[0m outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mdistribute_strategy\u001b[39m.\u001b[39;49mrun(run_step, args\u001b[39m=\u001b[39;49m(data,))\n\u001b[1;32m   1147\u001b[0m outputs \u001b[39m=\u001b[39m reduce_per_replica(\n\u001b[1;32m   1148\u001b[0m     outputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy, reduction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfirst\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1149\u001b[0m )\n\u001b[1;32m   1150\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniforge3/envs/iannwtf/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py:1315\u001b[0m, in \u001b[0;36mStrategyBase.run\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1310\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscope():\n\u001b[1;32m   1311\u001b[0m   \u001b[39m# tf.distribute supports Eager functions, so AutoGraph should not be\u001b[39;00m\n\u001b[1;32m   1312\u001b[0m   \u001b[39m# applied when the caller is also in Eager mode.\u001b[39;00m\n\u001b[1;32m   1313\u001b[0m   fn \u001b[39m=\u001b[39m autograph\u001b[39m.\u001b[39mtf_convert(\n\u001b[1;32m   1314\u001b[0m       fn, autograph_ctx\u001b[39m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m-> 1315\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_extended\u001b[39m.\u001b[39;49mcall_for_each_replica(fn, args\u001b[39m=\u001b[39;49margs, kwargs\u001b[39m=\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/iannwtf/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py:2891\u001b[0m, in \u001b[0;36mStrategyExtendedV1.call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   2889\u001b[0m   kwargs \u001b[39m=\u001b[39m {}\n\u001b[1;32m   2890\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_container_strategy()\u001b[39m.\u001b[39mscope():\n\u001b[0;32m-> 2891\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_for_each_replica(fn, args, kwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/iannwtf/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py:3692\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   3690\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call_for_each_replica\u001b[39m(\u001b[39mself\u001b[39m, fn, args, kwargs):\n\u001b[1;32m   3691\u001b[0m   \u001b[39mwith\u001b[39;00m ReplicaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_container_strategy(), replica_id_in_sync_group\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[0;32m-> 3692\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/iannwtf/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:689\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    688\u001b[0m   \u001b[39mwith\u001b[39;00m conversion_ctx:\n\u001b[0;32m--> 689\u001b[0m     \u001b[39mreturn\u001b[39;00m converted_call(f, args, kwargs, options\u001b[39m=\u001b[39;49moptions)\n\u001b[1;32m    690\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    691\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[0;32m~/miniforge3/envs/iannwtf/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:377\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    374\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[1;32m    376\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m options\u001b[39m.\u001b[39muser_requested \u001b[39mand\u001b[39;00m conversion\u001b[39m.\u001b[39mis_allowlisted(f):\n\u001b[0;32m--> 377\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[1;32m    379\u001b[0m \u001b[39m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[39m# call conversion from generated code while in nonrecursive mode. In that\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[39m# case we evidently don't want to recurse, but we still have to convert\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[39m# things like builtins.\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m options\u001b[39m.\u001b[39minternal_convert_user_code:\n",
      "File \u001b[0;32m~/miniforge3/envs/iannwtf/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:458\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    455\u001b[0m   \u001b[39mreturn\u001b[39;00m f\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m.\u001b[39mcall(args, kwargs)\n\u001b[1;32m    457\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 458\u001b[0m   \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    459\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs)\n",
      "File \u001b[0;32m~/miniforge3/envs/iannwtf/lib/python3.10/site-packages/keras/engine/training.py:1135\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function.<locals>.run_step\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_step\u001b[39m(data):\n\u001b[0;32m-> 1135\u001b[0m     outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain_step(data)\n\u001b[1;32m   1136\u001b[0m     \u001b[39m# Ensure counter is updated only if `train_step` succeeds.\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m     \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mcontrol_dependencies(_minimum_control_deps(outputs)):\n",
      "File \u001b[0;32m~/miniforge3/envs/iannwtf/lib/python3.10/site-packages/keras/engine/training.py:997\u001b[0m, in \u001b[0;36mModel.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_target_and_loss(y, loss)\n\u001b[1;32m    996\u001b[0m \u001b[39m# Run backwards pass.\u001b[39;00m\n\u001b[0;32m--> 997\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mminimize(loss, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainable_variables, tape\u001b[39m=\u001b[39;49mtape)\n\u001b[1;32m    998\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_metrics(x, y, y_pred, sample_weight)\n",
      "File \u001b[0;32m~/miniforge3/envs/iannwtf/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py:576\u001b[0m, in \u001b[0;36mOptimizerV2.minimize\u001b[0;34m(self, loss, var_list, grad_loss, name, tape)\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mminimize\u001b[39m(\u001b[39mself\u001b[39m, loss, var_list, grad_loss\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, tape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    546\u001b[0m     \u001b[39m\"\"\"Minimize `loss` by updating `var_list`.\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \n\u001b[1;32m    548\u001b[0m \u001b[39m    This method simply computes gradient using `tf.GradientTape` and calls\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    574\u001b[0m \n\u001b[1;32m    575\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 576\u001b[0m     grads_and_vars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute_gradients(\n\u001b[1;32m    577\u001b[0m         loss, var_list\u001b[39m=\u001b[39;49mvar_list, grad_loss\u001b[39m=\u001b[39;49mgrad_loss, tape\u001b[39m=\u001b[39;49mtape\n\u001b[1;32m    578\u001b[0m     )\n\u001b[1;32m    579\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_gradients(grads_and_vars, name\u001b[39m=\u001b[39mname)\n",
      "File \u001b[0;32m~/miniforge3/envs/iannwtf/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py:634\u001b[0m, in \u001b[0;36mOptimizerV2._compute_gradients\u001b[0;34m(self, loss, var_list, grad_loss, tape)\u001b[0m\n\u001b[1;32m    632\u001b[0m var_list \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(var_list)\n\u001b[1;32m    633\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mname_scope(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/gradients\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 634\u001b[0m     grads_and_vars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_gradients(\n\u001b[1;32m    635\u001b[0m         tape, loss, var_list, grad_loss\n\u001b[1;32m    636\u001b[0m     )\n\u001b[1;32m    638\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_assert_valid_dtypes(\n\u001b[1;32m    639\u001b[0m     [\n\u001b[1;32m    640\u001b[0m         v\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    643\u001b[0m     ]\n\u001b[1;32m    644\u001b[0m )\n\u001b[1;32m    646\u001b[0m \u001b[39mreturn\u001b[39;00m grads_and_vars\n",
      "File \u001b[0;32m~/miniforge3/envs/iannwtf/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py:510\u001b[0m, in \u001b[0;36mOptimizerV2._get_gradients\u001b[0;34m(self, tape, loss, var_list, grad_loss)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_gradients\u001b[39m(\u001b[39mself\u001b[39m, tape, loss, var_list, grad_loss\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    509\u001b[0m     \u001b[39m\"\"\"Called in `minimize` to compute gradients from loss.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 510\u001b[0m     grads \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39;49mgradient(loss, var_list, grad_loss)\n\u001b[1;32m    511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(grads, var_list))\n",
      "File \u001b[0;32m~/miniforge3/envs/iannwtf/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py:1113\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1107\u001b[0m   output_gradients \u001b[39m=\u001b[39m (\n\u001b[1;32m   1108\u001b[0m       composite_tensor_gradient\u001b[39m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[1;32m   1109\u001b[0m           output_gradients))\n\u001b[1;32m   1110\u001b[0m   output_gradients \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m x \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m ops\u001b[39m.\u001b[39mconvert_to_tensor(x)\n\u001b[1;32m   1111\u001b[0m                       \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m output_gradients]\n\u001b[0;32m-> 1113\u001b[0m flat_grad \u001b[39m=\u001b[39m imperative_grad\u001b[39m.\u001b[39;49mimperative_grad(\n\u001b[1;32m   1114\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tape,\n\u001b[1;32m   1115\u001b[0m     flat_targets,\n\u001b[1;32m   1116\u001b[0m     flat_sources,\n\u001b[1;32m   1117\u001b[0m     output_gradients\u001b[39m=\u001b[39;49moutput_gradients,\n\u001b[1;32m   1118\u001b[0m     sources_raw\u001b[39m=\u001b[39;49mflat_sources_raw,\n\u001b[1;32m   1119\u001b[0m     unconnected_gradients\u001b[39m=\u001b[39;49munconnected_gradients)\n\u001b[1;32m   1121\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_persistent:\n\u001b[1;32m   1122\u001b[0m   \u001b[39m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_watched_variables \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tape\u001b[39m.\u001b[39mwatched_variables()\n",
      "File \u001b[0;32m~/miniforge3/envs/iannwtf/lib/python3.10/site-packages/tensorflow/python/eager/imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     65\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mUnknown value for unconnected_gradients: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m unconnected_gradients)\n\u001b[0;32m---> 67\u001b[0m \u001b[39mreturn\u001b[39;00m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_TapeGradient(\n\u001b[1;32m     68\u001b[0m     tape\u001b[39m.\u001b[39;49m_tape,  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m     69\u001b[0m     target,\n\u001b[1;32m     70\u001b[0m     sources,\n\u001b[1;32m     71\u001b[0m     output_gradients,\n\u001b[1;32m     72\u001b[0m     sources_raw,\n\u001b[1;32m     73\u001b[0m     compat\u001b[39m.\u001b[39;49mas_str(unconnected_gradients\u001b[39m.\u001b[39;49mvalue))\n",
      "File \u001b[0;32m~/miniforge3/envs/iannwtf/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py:160\u001b[0m, in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    158\u001b[0m     gradient_name_scope \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m forward_pass_name_scope \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m   \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mname_scope(gradient_name_scope):\n\u001b[0;32m--> 160\u001b[0m     \u001b[39mreturn\u001b[39;00m grad_fn(mock_op, \u001b[39m*\u001b[39;49mout_grads)\n\u001b[1;32m    161\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m   \u001b[39mreturn\u001b[39;00m grad_fn(mock_op, \u001b[39m*\u001b[39mout_grads)\n",
      "File \u001b[0;32m~/miniforge3/envs/iannwtf/lib/python3.10/site-packages/tensorflow/python/ops/array_grad.py:830\u001b[0m, in \u001b[0;36m_TransposeGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    828\u001b[0m \u001b[39m\"\"\"Returns unshuffle(grad).\"\"\"\u001b[39;00m\n\u001b[1;32m    829\u001b[0m p \u001b[39m=\u001b[39m op\u001b[39m.\u001b[39minputs[\u001b[39m1\u001b[39m]\n\u001b[0;32m--> 830\u001b[0m \u001b[39mreturn\u001b[39;00m [array_ops\u001b[39m.\u001b[39;49mtranspose(grad, array_ops\u001b[39m.\u001b[39;49minvert_permutation(p)), \u001b[39mNone\u001b[39;00m]\n",
      "File \u001b[0;32m~/miniforge3/envs/iannwtf/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniforge3/envs/iannwtf/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/iannwtf/lib/python3.10/site-packages/tensorflow/python/ops/array_ops.py:2366\u001b[0m, in \u001b[0;36mtranspose\u001b[0;34m(a, perm, name, conjugate)\u001b[0m\n\u001b[1;32m   2363\u001b[0m   transpose_fn \u001b[39m=\u001b[39m gen_array_ops\u001b[39m.\u001b[39mtranspose\n\u001b[1;32m   2365\u001b[0m \u001b[39mif\u001b[39;00m perm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 2366\u001b[0m   \u001b[39mreturn\u001b[39;00m transpose_fn(a, perm, name\u001b[39m=\u001b[39;49mname)\n\u001b[1;32m   2368\u001b[0m rank \u001b[39m=\u001b[39m a\u001b[39m.\u001b[39mshape\u001b[39m.\u001b[39mrank\n\u001b[1;32m   2369\u001b[0m \u001b[39mif\u001b[39;00m rank \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/iannwtf/lib/python3.10/site-packages/tensorflow/python/ops/gen_array_ops.py:11887\u001b[0m, in \u001b[0;36mtranspose\u001b[0;34m(x, perm, name)\u001b[0m\n\u001b[1;32m  11885\u001b[0m     \u001b[39mpass\u001b[39;00m  \u001b[39m# Add nodes to the TensorFlow graph.\u001b[39;00m\n\u001b[1;32m  11886\u001b[0m \u001b[39m# Add nodes to the TensorFlow graph.\u001b[39;00m\n\u001b[0;32m> 11887\u001b[0m _, _, _op, _outputs \u001b[39m=\u001b[39m _op_def_library\u001b[39m.\u001b[39;49m_apply_op_helper(\n\u001b[1;32m  11888\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39mTranspose\u001b[39;49m\u001b[39m\"\u001b[39;49m, x\u001b[39m=\u001b[39;49mx, perm\u001b[39m=\u001b[39;49mperm, name\u001b[39m=\u001b[39;49mname)\n\u001b[1;32m  11889\u001b[0m _result \u001b[39m=\u001b[39m _outputs[:]\n\u001b[1;32m  11890\u001b[0m \u001b[39mif\u001b[39;00m _execute\u001b[39m.\u001b[39mmust_record_gradient():\n",
      "File \u001b[0;32m~/miniforge3/envs/iannwtf/lib/python3.10/site-packages/tensorflow/python/framework/op_def_library.py:797\u001b[0m, in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    792\u001b[0m must_colocate_inputs \u001b[39m=\u001b[39m [val \u001b[39mfor\u001b[39;00m arg, val \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(op_def\u001b[39m.\u001b[39minput_arg, inputs)\n\u001b[1;32m    793\u001b[0m                         \u001b[39mif\u001b[39;00m arg\u001b[39m.\u001b[39mis_ref]\n\u001b[1;32m    794\u001b[0m \u001b[39mwith\u001b[39;00m _MaybeColocateWith(must_colocate_inputs):\n\u001b[1;32m    795\u001b[0m   \u001b[39m# Add Op to graph\u001b[39;00m\n\u001b[1;32m    796\u001b[0m   \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m--> 797\u001b[0m   op \u001b[39m=\u001b[39m g\u001b[39m.\u001b[39;49m_create_op_internal(op_type_name, inputs, dtypes\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    798\u001b[0m                              name\u001b[39m=\u001b[39;49mscope, input_types\u001b[39m=\u001b[39;49minput_types,\n\u001b[1;32m    799\u001b[0m                              attrs\u001b[39m=\u001b[39;49mattr_protos, op_def\u001b[39m=\u001b[39;49mop_def)\n\u001b[1;32m    801\u001b[0m \u001b[39m# `outputs` is returned as a separate return value so that the output\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[39m# tensors can the `op` per se can be decoupled so that the\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[39m# `op_callbacks` can function properly. See framework/op_callbacks.py\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[39m# for more details.\u001b[39;00m\n\u001b[1;32m    805\u001b[0m outputs \u001b[39m=\u001b[39m op\u001b[39m.\u001b[39moutputs\n",
      "File \u001b[0;32m~/miniforge3/envs/iannwtf/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:735\u001b[0m, in \u001b[0;36mFuncGraph._create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m    733\u001b[0m   inp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcapture(inp)\n\u001b[1;32m    734\u001b[0m   captured_inputs\u001b[39m.\u001b[39mappend(inp)\n\u001b[0;32m--> 735\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(FuncGraph, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m_create_op_internal(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    736\u001b[0m     op_type, captured_inputs, dtypes, input_types, name, attrs, op_def,\n\u001b[1;32m    737\u001b[0m     compute_device)\n",
      "File \u001b[0;32m~/miniforge3/envs/iannwtf/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:3809\u001b[0m, in \u001b[0;36mGraph._create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   3799\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mutation_lock():\n\u001b[1;32m   3800\u001b[0m   ret \u001b[39m=\u001b[39m Operation(\n\u001b[1;32m   3801\u001b[0m       node_def,\n\u001b[1;32m   3802\u001b[0m       \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3807\u001b[0m       original_op\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_default_original_op,\n\u001b[1;32m   3808\u001b[0m       op_def\u001b[39m=\u001b[39mop_def)\n\u001b[0;32m-> 3809\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_op_helper(ret, compute_device\u001b[39m=\u001b[39;49mcompute_device)\n\u001b[1;32m   3810\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/miniforge3/envs/iannwtf/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:3884\u001b[0m, in \u001b[0;36mGraph._create_op_helper\u001b[0;34m(self, op, compute_device)\u001b[0m\n\u001b[1;32m   3881\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_record_op_seen_by_control_dependencies(op)\n\u001b[1;32m   3883\u001b[0m \u001b[39mif\u001b[39;00m compute_device:\n\u001b[0;32m-> 3884\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply_device_functions(op)\n\u001b[1;32m   3886\u001b[0m \u001b[39m# Snapshot the colocation stack metadata before we might generate error\u001b[39;00m\n\u001b[1;32m   3887\u001b[0m \u001b[39m# messages using it.  Note that this snapshot depends on the actual stack\u001b[39;00m\n\u001b[1;32m   3888\u001b[0m \u001b[39m# and is independent of the op's _class attribute.\u001b[39;00m\n\u001b[1;32m   3889\u001b[0m \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   3890\u001b[0m op\u001b[39m.\u001b[39m_colocation_code_locations \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_snapshot_colocation_stack_metadata()\n",
      "File \u001b[0;32m~/miniforge3/envs/iannwtf/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:4791\u001b[0m, in \u001b[0;36mGraph._apply_device_functions\u001b[0;34m(self, op)\u001b[0m\n\u001b[1;32m   4785\u001b[0m \u001b[39m# Apply any device functions in LIFO order, so that the most recently\u001b[39;00m\n\u001b[1;32m   4786\u001b[0m \u001b[39m# pushed function has the first chance to apply a device to the op.\u001b[39;00m\n\u001b[1;32m   4787\u001b[0m \u001b[39m# We apply here because the result can depend on the Operation's\u001b[39;00m\n\u001b[1;32m   4788\u001b[0m \u001b[39m# signature, which is computed in the Operation constructor.\u001b[39;00m\n\u001b[1;32m   4789\u001b[0m \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   4790\u001b[0m prior_device_string \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 4791\u001b[0m \u001b[39mfor\u001b[39;00m device_spec \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_device_function_stack\u001b[39m.\u001b[39;49mpeek_objs():\n\u001b[1;32m   4792\u001b[0m   \u001b[39mif\u001b[39;00m device_spec\u001b[39m.\u001b[39mis_null_merge:\n\u001b[1;32m   4793\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/iannwtf/lib/python3.10/site-packages/tensorflow/python/framework/traceable_stack.py:111\u001b[0m, in \u001b[0;36mTraceableStack.peek_objs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpeek_objs\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    110\u001b[0m   \u001b[39m\"\"\"Return iterator over stored objects ordered newest to oldest.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m   \u001b[39mreturn\u001b[39;00m (t_obj\u001b[39m.\u001b[39mobj \u001b[39mfor\u001b[39;00m t_obj \u001b[39min\u001b[39;00m \u001b[39mreversed\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stack))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "HP_MODEL_ARCHITECTURE = hp.HParam('model_architecture', hp.Discrete(['BidirectionalLSTM'])) # set model architecture as hyperparameter\n",
    "HP_NUM_LSTM_LAYERS = hp.HParam('num_lstm_layers', hp.Discrete([1, 2])) # try out 1, 2 or 4 layers for architecture\n",
    "HP_NUM_HIDDEN_UNITS = hp.HParam('num_hidden_units', hp.Discrete([8,32])) # try out different hidden_units\n",
    "HP_NUM_CONV_LAYERS = hp.HParam('num_conv_layers', hp.Discrete([1])) # try out different amounts of conv layers --> connected to filter size\n",
    "HP_NUM_DENSE_UNITS = hp.HParam('num_dense_units', hp.Discrete([640])) # try out different number of units for dense layer\n",
    "HP_NUM_DENSE_LAYERS = hp.HParam('num_dense_layers', hp.Discrete([0,1])) # try out different number of dense layers\n",
    "HP_INCREASE_UNITS_PER_LSTM_LAYER = hp.HParam('increase_units_per_lstm_layer', hp.Discrete([0])) # increase number of units per lstm layer\n",
    "\n",
    "hparams_dict_5 = {\n",
    "    'HP_MODEL_ARCHITECTURE': HP_MODEL_ARCHITECTURE,\n",
    "    'HP_NUM_LSTM_LAYERS': HP_NUM_LSTM_LAYERS,\n",
    "    'HP_NUM_HIDDEN_UNITS': HP_NUM_HIDDEN_UNITS,\n",
    "    'HP_NUM_CONV_LAYERS': HP_NUM_CONV_LAYERS,\n",
    "    'HP_NUM_DENSE_UNITS': HP_NUM_DENSE_UNITS,\n",
    "    'HP_NUM_DENSE_LAYERS': HP_NUM_DENSE_LAYERS,\n",
    "    'HP_INCREASE_UNITS_PER_LSTM_LAYER': HP_INCREASE_UNITS_PER_LSTM_LAYER\n",
    "}\n",
    "\n",
    "search(logdir='logs1-5/', hparams_dict=hparams_dict_5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "IFD-TSa9l6Tc",
    "9qo8QSn3q7tD",
    "JPmB8Wh1na2r",
    "jBbZmkDYj9xD",
    "j948o_LdwExJ",
    "i255ym_VodSF"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "a3449bbb043929c6f13b514689ff91c66257e0787e2d8bb0eba8270d3f40eacf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
